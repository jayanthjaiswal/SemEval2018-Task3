{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS289 Final Project: Irony Detection in English Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team: Jayanth, Sudharsan Krishnaswamy, Debleena Sengupta, Shadi Shahsavari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advent of social media like Twitter and Facebook has led to rise of people using more creative and figurative language use like Irony, Sarcasm, Hyperbole etc to catch the social network’s attention for more likes and retweets. Natural Language Processing Tasks on such social media datasets like Sentiment Analysis, Opinion Mining, Argument Analysis etc struggle to maintain high performance, when applied to Ironic texts. We try to tackle this hard problem of Irony Detection using new advances in Deep Learning technologies. We approach the first tasks in our work based on SemEval 2018 dataset. The first task is to detect if a tweet is “ironic” or not (binary classification of 0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SemEval Dataset Preprocessing and Corpus Analysis with Visual Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Implementations of traditional ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Model to Capture Linguistic Property of Irony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Algorithm Implementation and Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Implementation and Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the deep learning model, we explored a boosting implementation. The idea was to use the same word embedding features that were used in the DNN in teh boosting algorithm to see if we could achieve a letter performance. Below is the implementation of our boosting algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#preprocess data files that contain word_embedding features.\n",
    "#May need to change path based on where data is\n",
    "data_y_fp = \"./labels.txt\"\n",
    "directory = \"word_embeddings\"\n",
    "MAX_LENGTH = 310\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "#go through labels file:\n",
    "with open(data_y_fp) as f:\n",
    "    for label in f:\n",
    "        label = int(label)\n",
    "        data_y.append(label)\n",
    "data_y = np.array(data_y)\n",
    "\n",
    "#go through the feature files. Each file has 25 features\n",
    "files = os.listdir(directory)\n",
    "os.chdir(directory)\n",
    "for filename in files:\n",
    "    example = []\n",
    "    name = filename.split(\".\")\n",
    "    file_num = int(name[0])\n",
    "    label = data_y[file_num-1]\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            tmp = line.split(\" \")\n",
    "            tmp = tmp[:len(tmp)-1]\n",
    "            tmp = map(float, tmp)\n",
    "            example = example + tmp\n",
    "    example = np.array(example)\n",
    "    if len(example)>MAX_LENGTH:\n",
    "        example = example[:MAX_LENGTH]\n",
    "    if len(example)<MAX_LENGTH:\n",
    "        example = np.pad(example, (0,MAX_LENGTH-len(example)), 'constant')\n",
    "    example = np.append(example, label)\n",
    "    data_x.append(example)\n",
    "data_x = np.array(data_x)\n",
    "X = data_x[:,0:MAX_LENGTH]\n",
    "Y = data_x[:,MAX_LENGTH]\n",
    "\n",
    "seed = 7\n",
    "test_size = 0.1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=test_size, random_state=seed)\n",
    "# fit model no training data\n",
    "model = XGBRegressor(max_depth=3) #gave 56.51%\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Accuracy: 58.33%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the implementation, we decided to use the XGBRegressor from the xgboost library. This is a classifier uses a logistic regression model to perform the binary classification task. Based on the history of the performance of boosting, we hypothesized that with the proper feature selection, boosting would acheive a significntly high score. A variety of feature implementations were tested. We tried different combinations of sentiment score extractions and we tried word embedding. Out of all of the techniques, the word embedding features performed the best, resulting in a 58.33% accuracy, as listed above. We also present the code for the most successful accuracy output. Even though this score was better than random guessing, it did not outperform the DNN approach, as was expected. After careful deliberation, we have come to the conclusion that boosting only works successfully when the proper features are selected (?). In the case of detecting irony, only extracting sentiment scores or word embeddings were not strong enough features to characterize a tweet as ironic or not ironic.\n",
    "\n",
    "Describe code, describe tuning, describe 10 fold testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Analysis and Test Data Evaluation Submission on SemEval Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
