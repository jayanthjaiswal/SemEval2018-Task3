{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS289 Final Project: Irony Detection in English Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team: Jayanth, Sudharsan Krishnaswamy, Debleena Sengupta, Shadi Shahsavari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advent of social media like Twitter and Facebook has led to rise of people using more creative and figurative language use like Irony, Sarcasm, Hyperbole etc to catch the social network‚Äôs attention for more likes and retweets. Natural Language Processing Tasks on such social media datasets like Sentiment Analysis, Opinion Mining, Argument Analysis etc struggle to maintain high performance, when applied to Ironic texts. We try to tackle this hard problem of Irony Detection using new advances in Deep Learning technologies. We approach the first tasks in our work based on SemEval 2018 dataset. The first task is to detect if a tweet is ‚Äúironic‚Äù or not (binary classification of 0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SemEval Dataset Preprocessing and Corpus Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was processed to ambiguate the urls as URL and the usernames as USER using regular expression. Also, the emojis were converted to their unicode equivalent aliases as defined by the [unicode consortium](http://www.unicode.org/emoji/charts/full-emoji-list.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USR: One day I want to travel with my bestfriend :globe_showing_Asia-Australia: :airplane:Ô∏è URL DONE DID TRAVELED DA WORLD!! USR :black_heart: \n"
     ]
    }
   ],
   "source": [
    "import re,emoji\n",
    "sent = u'@SincerelyTumblr: One day I want to travel with my bestfriend üåè ‚úàÔ∏è http://t.co/AXD3Ax5qC1 DONE DID TRAVELED DA WORLD!! @Bethanycsmithh üñ§ '\n",
    "sent = emoji.demojize(sent)\n",
    "sent = re.sub(r'https?:\\/\\/[^ \\n\\t\\r]*', 'URL', sent)\n",
    "sent = re.sub(r'@[a-zA-Z0-9_]+', 'USR', sent);\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocations are expressions of multiple words which commonly co-occur. They give important insight into the common patterns in both classes. A number of measures are available in NLTK to score collocations or other associations. The collocations in the corpus were then, scored based on those metrices and ranked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True).tokenize\n",
    "tokens = tokenizer('\\n'.join(corpus))\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "scored = finder.score_ngrams(bigram_measures.chi_sq)\n",
    "sorted(bigram for bigram, score in scored)\n",
    "map(lambda x: print(' '.join(x[0]), x[1], \"\\n\"), scored[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocation Scoring\n",
    "\n",
    "| Raw Freq | Chi_Sq/Dice/Jaccard/Phi_Sq/pmi     | Likelihood Ratio          |\n",
    "|----------|------------------------------------|---------------------------|\n",
    "| ! !      | #034i #100                         | ! !                       |\n",
    "| in the   | #100 #glitter                      | i love                    |\n",
    "| . i      | #1stphoto #rektek                  | I I                       |\n",
    "| i love   | #2003 2003                         | going to                  |\n",
    "| I I      | #2015isthenewturnup #myboos        | to be                     |\n",
    "| to be    | #2015season #2014sucks             | face_with_tears_of_joy :: |\n",
    "| of the   | #2am http://t.co/49XwyrlADo        | in the                    |\n",
    "| . I      | #2n1edition http://t.co/oRT6ZYfGhx | can't wait                |\n",
    "| for the  | #2o14 #bestie                      | :: face_with_tears_of_joy |\n",
    "| to the   | #2of6 #6daystretch                 | : Ô∏è                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Implementations of traditional ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize(corpus):\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True).tokenize\n",
    "    vectorizer = TfidfVectorizer(strip_accents=\"unicode\", analyzer=\"word\", tokenizer=tokenizer, stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_informative_feature_for_binary_classification(vectorizer, classifier, n=10):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.coef_[0], feature_names))[:n]\n",
    "    topn_class2 = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n",
    "\n",
    "    for coef, feat in topn_class1:\n",
    "        print (class_labels[0], coef, feat)\n",
    "    print ()\n",
    "    for coef, feat in reversed(topn_class2):\n",
    "        print (class_labels[1], coef, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    K_FOLDS = 10 # 10-fold crossvalidation\n",
    "    CLF = LinearSVC() # the default, non-parameter optimized linear-kernel SVM\n",
    "    corpus, y = parse_dataset(DATASET_FP) # Loading dataset and featurised simple Tfidf-BoW model\n",
    "    X = featurize(corpus)\n",
    "\n",
    "    class_counts = np.asarray(np.unique(y, return_counts=True)).T.tolist()\n",
    "    \n",
    "    # Returns an array of the same size as 'y' where each entry is a prediction obtained by cross validated\n",
    "    predicted = cross_val_predict(CLF, X, y, cv=K_FOLDS)\n",
    "    \n",
    "    # Modify F1-score calculation depending on the task\n",
    "    if TASK.lower() == 'a':\n",
    "        score = metrics.f1_score(y, predicted, pos_label=1)\n",
    "    elif TASK.lower() == 'b':\n",
    "        score = metrics.f1_score(y, predicted, average=\"macro\")\n",
    "    print (\"F1-score Task\", TASK, score)\n",
    "    for p in predicted:\n",
    "        PREDICTIONSFILE.write(\"{}\\n\".format(p))\n",
    "    PREDICTIONSFILE.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Model to Capture Linguistic Property of Irony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Algorithm Implementation and Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Implementation and Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the deep learning model, we explored a boosting implementation. The idea was to use the same word embedding features that were used in the DNN in teh boosting algorithm to see if we could achieve a letter performance. Below is the implementation of our boosting algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named xgboost",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-633620c6f3b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named xgboost"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#preprocess data files that contain word_embedding features.\n",
    "#May need to change path based on where data is\n",
    "data_y_fp = \"./labels.txt\"\n",
    "directory = \"word_embeddings\"\n",
    "MAX_LENGTH = 310\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "#go through labels file:\n",
    "with open(data_y_fp) as f:\n",
    "    for label in f:\n",
    "        label = int(label)\n",
    "        data_y.append(label)\n",
    "data_y = np.array(data_y)\n",
    "\n",
    "#go through the feature files. Each file has 25 features\n",
    "files = os.listdir(directory)\n",
    "os.chdir(directory)\n",
    "for filename in files:\n",
    "    example = []\n",
    "    name = filename.split(\".\")\n",
    "    file_num = int(name[0])\n",
    "    label = data_y[file_num-1]\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            tmp = line.split(\" \")\n",
    "            tmp = tmp[:len(tmp)-1]\n",
    "            tmp = map(float, tmp)\n",
    "            example = example + tmp\n",
    "    example = np.array(example)\n",
    "    if len(example)>MAX_LENGTH:\n",
    "        example = example[:MAX_LENGTH]\n",
    "    if len(example)<MAX_LENGTH:\n",
    "        example = np.pad(example, (0,MAX_LENGTH-len(example)), 'constant')\n",
    "    example = np.append(example, label)\n",
    "    data_x.append(example)\n",
    "data_x = np.array(data_x)\n",
    "X = data_x[:,0:MAX_LENGTH]\n",
    "Y = data_x[:,MAX_LENGTH]\n",
    "\n",
    "seed = 7\n",
    "test_size = 0.1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=test_size, random_state=seed)\n",
    "# fit model no training data\n",
    "model = XGBRegressor(max_depth=3) #gave 56.51%\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Accuracy: 58.33%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the implementation, we decided to use the XGBRegressor from the xgboost library. This is a classifier uses a logistic regression model to perform the binary classification task. Based on the history of the performance of boosting, we hypothesized that with the proper feature selection, boosting would acheive a significntly high score. A variety of feature implementations were tested. We tried different combinations of sentiment score extractions and we tried word embedding. Out of all of the techniques, the word embedding features performed the best, resulting in a 58.33% accuracy, as listed above. We also present the code for the most successful accuracy output. Even though this score was better than random guessing, it did not outperform the DNN approach, as was expected. After careful deliberation, we have come to the conclusion that boosting only works successfully when the proper features are selected (?). In the case of detecting irony, only extracting sentiment scores or word embeddings were not strong enough features to characterize a tweet as ironic or not ironic.\n",
    "\n",
    "Describe code, describe tuning, describe 10 fold testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Analysis and Test Data Evaluation Submission on SemEval Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
