{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS289 Final Project: Irony Detection in English Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team: Jayanth, Sudharsan Krishnaswamy, Debleena Sengupta, Shadi Shahsavari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advent of social media like Twitter and Facebook has led to rise of people using more creative and figurative language use like Irony, Sarcasm, Hyperbole etc to catch the social networkâ€™s attention for more likes and retweets. Natural Language Processing Tasks on such social media datasets like Sentiment Analysis, Opinion Mining, Argument Analysis etc struggle to maintain high performance, when applied to Ironic texts. We try to tackle this hard problem of Irony Detection using new advances in Deep Learning technologies. We approach the first tasks in our work based on SemEval 2018 dataset. The first task is to detect if a tweet is â€œironicâ€ or not (binary classification of 0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SemEval Dataset Preprocessing and Corpus Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was processed to ambiguate the urls as URL and the usernames as USER using regular expression. Also, the emojis were converted to their unicode equivalent aliases as defined by the [unicode consortium](http://www.unicode.org/emoji/charts/full-emoji-list.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USR: One day I want to travel with my bestfriend :globe_showing_Asia-Australia: :airplane:ï¸ URL DONE DID TRAVELED DA WORLD!! USR :black_heart: \n"
     ]
    }
   ],
   "source": [
    "import re,emoji\n",
    "sent = u'@SincerelyTumblr: One day I want to travel with my bestfriend ðŸŒ âœˆï¸ http://t.co/AXD3Ax5qC1 DONE DID TRAVELED DA WORLD!! @Bethanycsmithh ðŸ–¤ '\n",
    "sent = emoji.demojize(sent)\n",
    "sent = re.sub(r'https?:\\/\\/[^ \\n\\t\\r]*', 'URL', sent)\n",
    "sent = re.sub(r'@[a-zA-Z0-9_]+', 'USR', sent);\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocations are expressions of multiple words which commonly co-occur. They give important insight into the common patterns in both classes. A number of measures are available in NLTK to score collocations or other associations. The collocations in the corpus were then, scored based on those metrices and ranked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True).tokenize\n",
    "tokens = tokenizer('\\n'.join(corpus))\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "scored = finder.score_ngrams(bigram_measures.chi_sq)\n",
    "sorted(bigram for bigram, score in scored)\n",
    "map(lambda x: print(' '.join(x[0]), x[1], \"\\n\"), scored[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocation Scoring\n",
    "\n",
    "| Raw Freq | Chi_Sq/Dice/Jaccard/Phi_Sq/pmi     | Likelihood Ratio          |\n",
    "|----------|------------------------------------|---------------------------|\n",
    "| ! !      | #034i #100                         | ! !                       |\n",
    "| in the   | #100 #glitter                      | i love                    |\n",
    "| . i      | #1stphoto #rektek                  | I I                       |\n",
    "| i love   | #2003 2003                         | going to                  |\n",
    "| I I      | #2015isthenewturnup #myboos        | to be                     |\n",
    "| to be    | #2015season #2014sucks             | face_with_tears_of_joy :: |\n",
    "| of the   | #2am http://t.co/49XwyrlADo        | in the                    |\n",
    "| . I      | #2n1edition http://t.co/oRT6ZYfGhx | can't wait                |\n",
    "| for the  | #2o14 #bestie                      | :: face_with_tears_of_joy |\n",
    "| to the   | #2of6 #6daystretch                 | : ï¸                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer [this spreadsheet](https://docs.google.com/spreadsheets/d/1SzHaj5J7IedYPns2vLK2_3aX85TJNJwuLoMNe5Vz670/edit#gid=1691220959) to look at top 10 collocations using different metrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Implementations of traditional ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried out two different N-gram language models as features:\n",
    "1. [Unigram](http://www.nltk.org/_modules/nltk/model/ngram.html)\n",
    "2. [Bigram](http://www.nltk.org/_modules/nltk/model/ngram.html)\n",
    "\n",
    "Those featues were then, converted to one of the following encodings for training:\n",
    "1. [Bag of Words](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer)\n",
    "2. [Count](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)\n",
    "3. [Tf-idf](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "\n",
    "Following is the code for featurizing the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize(corpus):\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True).tokenize\n",
    "    vectorizer = TfidfVectorizer(strip_accents=\"unicode\", analyzer=\"word\", tokenizer=tokenizer, stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried out multiple ML classifiers with different features and also, did comparative study.\n",
    "The following classifiers were tried:\n",
    "\n",
    "1. [Naive Bayes](http://www.nltk.org/api/nltk.classify.html#module-nltk.classify.naivebayes)\n",
    "2. [Decision Tree](http://www.nltk.org/api/nltk.classify.html#module-nltk.classify.decisiontree)\n",
    "3. [MaxEnt](http://www.nltk.org/api/nltk.classify.html#module-nltk.classify.maxent)\n",
    "4. [GIS](http://www.nltk.org/api/nltk.classify.html#module-nltk.classify.maxent)\n",
    "5. [IIS](http://www.nltk.org/api/nltk.classify.html#module-nltk.classify.maxent)\n",
    "6. [Bernoulli Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)\n",
    "7. [Decision Tree Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "8. [Extra Trees Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)\n",
    "9. [Gausian NaÃ¯ve Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "10. [Gradient Boosting Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "11. [K Nearest Neighbours Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "12. [Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "13. [Linear Support Vector Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)\n",
    "14. [Multinomial NaÃ¯ve Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "15. [Nu-Support Vector Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html)\n",
    "16. [Random Forest Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "17. [Support Vector Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "\n",
    "Following is the code for using the classifiers and doing [10-fold cross-validation](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    K_FOLDS = 10 # 10-fold crossvalidation\n",
    "    CLF = LinearSVC() # the default, non-parameter optimized linear-kernel SVM\n",
    "    corpus, y = parse_dataset(DATASET_FP) # Loading dataset and featurised simple Tfidf-BoW model\n",
    "    X = featurize(corpus)\n",
    "    # Returns an array of the same size as 'y' where each entry is a prediction obtained by cross validated\n",
    "    predicted = cross_val_predict(CLF, X, y, cv=K_FOLDS)\n",
    "    score = metrics.f1_score(y, predicted, pos_label=1)\n",
    "    print (\"F1-score Task\", TASK, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Comparision\n",
    "\n",
    "Refer [this spreadsheet](https://docs.google.com/spreadsheets/d/1x-MsT3iaUSs85UQPOhBzIOWSe4DOmhXs7M9TqUs_QJ4/edit#gid=256991540) for complete comparision.\n",
    "\n",
    "### Bigram does not improve much\n",
    "\n",
    "### Tf-idf performs the best\n",
    "\n",
    "### Nu-SVC, Naive Bayes and Logistic Regression give comparativeley better performance than all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also found the most informative features for the classifications to see, which words were important in distinguishing both class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_informative_feature_for_binary_classification(vectorizer, classifier, n=10):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.coef_[0], feature_names))[:n]\n",
    "    topn_class2 = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n",
    "\n",
    "    for coef, feat in topn_class1:\n",
    "        print (class_labels[0], coef, feat)\n",
    "    print ()\n",
    "    for coef, feat in reversed(topn_class2):\n",
    "        print (class_labels[1], coef, feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Naive Bayes                                                     | SVC                               | Linear SVC                        | MaxEnt                                        |\n",
    "|-----------------------------------------------------------------|-----------------------------------|-----------------------------------|-----------------------------------------------|\n",
    "| waking = 1               irony : non-ir =     14.4 : 1.0        | irony 3.43705831007 love          | irony 2.09798273149 unamused_face | 0.892 swag==1 and label is 'irony'            |\n",
    "| final = 1               irony : non-ir =      9.7 : 1.0         | irony 3.14001851865 great         | irony 1.9912945247 love           | 0.892 illridewithyou==1 and label is 'irony'  |\n",
    "| understand = 1              non-ir : irony  =      8.3 : 1.0    | irony 2.68216438685 fun           | irony 1.95849441798 great         | 0.866 speakeasy==1 and label is 'non-irony'   |\n",
    "| yay = 1               irony : non-ir =      7.6 : 1.0           | irony 2.10512791389 unamused_face | irony 1.89300320932 yay           | 0.631 socialist==1 and label is 'irony'       |\n",
    "| unamused_face = 1               irony : non-ir =      7.0 : 1.0 | irony 1.94194114863 yay           | irony 1.56713777847 joy           | -0.609 waking==1 and label is 'non-irony'     |\n",
    "| fix = 1               irony : non-ir =      6.4 : 1.0           | irony 1.75237273405 nice          | irony 1.55896446035 monday        | 0.604 amazeballs==1 and label is 'irony'      |\n",
    "| have = 2               irony : non-ir =      6.4 : 1.0          | irony 1.7355600331 thanks         | irony 1.52283732966 nice          | 0.604 peak==1 and label is 'non-irony'        |\n",
    "| check = 1              non-ir : irony  =      6.1 : 1.0         | irony 1.6065307335 wow            | irony 1.5100562397 fun            | 0.600 kmyb19hr==1 and label is 'non-irony'    |\n",
    "| wonderful = 1               irony : non-ir =      5.8 : 1.0     | irony 1.60091530592 oh            | non-irony -1.50574938325 check    | 0.597 subtweeting==1 and label is 'non-irony' |\n",
    "| dont = 1              non-ir : irony  =      5.8 : 1.0          | non-irony -1.57302071027 :        | irony 1.47746195477 test          | 0.593 bliss==1 and label is 'irony'           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Model to Capture Linguistic Property of Irony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Algorithm Implementation and Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Implementation and Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Boosting:\n",
    "Based on the history of the performance of boosting, we hypothesized that with the proper feature selection, boosting would achieve a significantly high accuracy score. In addition to the deep learning model, we explored a boosting implementation. The idea was to use the same word embedding features that were used in the DNN in the boosting algorithm to see if we could achieve a better performance. A variety of feature implementations were tested. The features included sentiment score extractions, word embeddings, and a combination of the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Score Features:\n",
    "\n",
    "It was hypothesized that ironic tweets would have words with starkly contrasting sentiment scores. Given a corpus, each tweet was tokenized. Each word in the tokenized list was passed through a sentiment analysis tool (nltk sentiwordnet) and the positive and negative scores of the word was kept track of. Looking at groups of 3 words at a time, the maximum positive sentiment score and the maximum negative sentiment score in the 3 word window was determined and the two values were subtracted to represent how \"far away\" the sentiments of these words were from each other. The number of 3-word chunks that were observed was a hyperparameter MAX_LEN_2 (shown in the code below) that was tuned.\n",
    "\n",
    "The full code for boosting with sentiment score features can be found at: https://github.com/jayanthjaiswal/SemEval2018-Task3/blob/master/xgboost_test/boost_test.py\n",
    "\n",
    "The code listed below demonstrates how the sentiment features were constructed for each tweet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN_2 = 6\n",
    "def get_sent_max_pos_neg(corpus):\n",
    "    for curr_sentence in corpus:\n",
    "        curr_sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', ' ', curr_sentence)\n",
    "        curr_sentence = re.sub(r'@[a-zA-Z0-9_]+', ' ', curr_sentence)\n",
    "        sentence_tokenized = tokenizer(curr_sentence)\n",
    "        sent_list = []\n",
    "        for i in range(len(sentence_tokenized)-2):\n",
    "            curr_word = sentence_tokenized[i]\n",
    "            next_word = sentence_tokenized[i+1]\n",
    "            next_next = sentence_tokenized[i+2]\n",
    "            if curr_word.startswith('#'):\n",
    "                curr_word = curr_word[1:]\n",
    "            if next_word.startswith('#'):\n",
    "                next_word = next_word[1:]\n",
    "            if next_next.startswith('#'):\n",
    "                next_next = next_next[1:]\n",
    "            curr_senti_synsets = swn.senti_synsets(curr_word)\n",
    "            next_senti_synsets = swn.senti_synsets(next_word)\n",
    "            next_next_senti_synsets = swn.senti_synsets(next_next)\n",
    "            if len(curr_senti_synsets) > 0 and len(next_senti_synsets) > 0 and len(next_next_senti_synsets) > 0:\n",
    "                curr_pos = curr_senti_synsets[0].pos_score()\n",
    "                curr_neg = curr_senti_synsets[0].neg_score()\n",
    "                next_pos = next_senti_synsets[0].pos_score()\n",
    "                next_neg = next_senti_synsets[0].neg_score()\n",
    "                next_next_pos = next_next_senti_synsets[0].pos_score()\n",
    "                next_next_neg = next_next_senti_synsets[0].neg_score()\n",
    "                max_pos = max(curr_pos, next_pos, next_next_pos)\n",
    "                max_neg = max(curr_neg, next_neg, next_next_neg)\n",
    "                curr_sent = max_pos - max_neg\n",
    "                if curr_sent != 0:\n",
    "                    sent_list.append(curr_sent)\n",
    "        if len(sent_list) > MAX_LEN_2:\n",
    "            sent_list = sent_list[:MAX_LEN_2]\n",
    "        for i in range(MAX_LEN_2 - len(sent_list)):\n",
    "            sent_list.append(0.0)\n",
    "        inp_X.append(sent_list)\n",
    "    return inp_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding Features:\n",
    "\n",
    "Since simply using sentiment scores did not perform very well, exploring word embedding features was the next approach. We utilized the pre-trained word embeddings called GloVe from Stanford and outputted two sets of features. For the first set, each tweet corresponded to a text file where each row represented a word in the tweet using a 25d word embedding. For the second set, each tweet corresponded to a text file where each row represented a word in the tweet using a 50d word embedding.  \n",
    "\n",
    "The first task was to preprocess the text files by flattening each text file into a 1 dimensinal array representing the word embedding for approximately the first 10 words in a tweet. The number of words that were included in the flattened word embedding vector was a hyperparameter, MAX_LEN (shown in the code below). Each tweet example was represented as a 1  x 250 dimensional vector. This way, the examples were stacked together to get a full input training data matrix (data_X in the code below) with dimensions 2834 X 251 (last column was the labels). \n",
    "\n",
    "The same was done for the 50d word embeddings, resulting in an input training data matrix (data_X in the code below) with dimensions 2834 X 501. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting Implementation\n",
    "Once the data was preprocessed with the sentiment score features and word embedding features (stored in data_x in code below), the training data was split into X_train, X_test, y_train and y_test (as shown in the code below). Xgboost was run to fit a model to the training data. We used the XGBRegressor from the xgboost library. This was a classifier used a logistic regression model to perform the binary classification task. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 350\n",
    "test_size = 0.1\n",
    "X = data_x[:,0:MAX_LENGTH+MAX_LEN_2]\n",
    "Y = data_x[:,MAX_LENGTH+MAX_LEN_2]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=test_size)\n",
    "# fit model no training data\n",
    "model = XGBRegressor(max_depth=3) #gave 56.51%\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracies.append(accuracy)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full code for boosting with word embeddings and sentiment score features can be found at: https://github.com/jayanthjaiswal/SemEval2018-Task3/blob/master/xgboost_test/boost_test_2.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering, Hyperparameters, Accuracy Analysis\n",
    "\n",
    "A combination of word embedding features and sentiment score features were used to analyze which would produce the best accuracy scores. The main parameters that were tuned in the model were MAX_LEN, MAX_LEN_2, test_size (representing the k_fold), and the max_depth of the decision trees in the boosting model. The best results were achieved with test_size = 0.1 and the max_depth = 3 for all models. For the models involving the 25d word embedding features, MAX_LEN = 350. For the models involving the 50d word embedding features, MAX_LEN = 550. For models involving sentiment scores MAX_LEN_2 = 6.\n",
    "\n",
    "The findings of the best scores along with the corresponding hyperparameters are summarized in the chart below:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Feature Types                                    Accuracy       MAX_LEN   MAX_LEN_2    test_size    max_depth      \n",
    "1. Only Sentiment Scores                         54.95%         n/a       6            0.1          3\n",
    "2. Only 25d Word Embedding                       61.20%         350       n/a          0.1          3\n",
    "3. Only 50d Word Embedding                       61.98%         550       n/a          0.1          3\n",
    "4. 25d Word Embedding + Sentiment Scores         62.50%         350       6            0.1          3\n",
    "5. 50d Word Embedding + Sentiment Scores         60.16%         550       6            0.1          3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting Summary\n",
    "Even though we hypothesized that boosting would be a simple method to increase performance scores, it was observed that it was unable to beat the DNN approaches. The highest that was achieved using the boosting method was 62.50%, using 25d word embedding and sentiment score features. We concluded that in order to observe good performance for a boosting algorithm, proper feature selection is key. Feature selection in the task of classifying irony is complex and required more sophisticated systems such as DNNs, as opposed to the boosting approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Analysis and Test Data Evaluation Submission on SemEval Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
