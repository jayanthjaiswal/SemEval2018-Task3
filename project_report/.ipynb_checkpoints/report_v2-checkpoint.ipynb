{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS269 Final Project: Irony Detection in English Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team: Jayanth, Sudharsan Krishnaswamy, Debleena Sengupta, Shadi Shahsavari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our final project, we have explored the task of irony detection in Tweets (semEval 2018 - Task 3). The advent of social media like Twitter and Facebook has led to rise of people using more creative and figurative language use like Irony, Sarcasm, Hyperbole etc to catch the social network‚Äôs attention for more likes and retweets. Natural Language Processing Tasks on such social media datasets like Sentiment Analysis, Opinion Mining, and Argument Analysis struggle to maintain high performance, when applied to Ironic texts. We try to tackle this hard problem of Irony Detection using boosting and new advances in Deep Learning technologies. We approach the first tasks in our work based on SemEval 2018 dataset. The task is to detect if a tweet is ‚Äúironic‚Äù or not (binary classification of 0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SemEval Dataset Preprocessing and Corpus Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was processed to ambiguate the urls as URL and the usernames as USER using regular expression. Also, the emojis were converted to their unicode equivalent aliases as defined by the [unicode consortium](http://www.unicode.org/emoji/charts/full-emoji-list.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USR: One day I want to travel with my bestfriend :globe_showing_Asia-Australia: :airplane:Ô∏è URL DONE DID TRAVELED DA WORLD!! USR :black_heart: \n"
     ]
    }
   ],
   "source": [
    "import re,emoji\n",
    "sent = u'@SincerelyTumblr: One day I want to travel with my bestfriend üåè ‚úàÔ∏è http://t.co/AXD3Ax5qC1 DONE DID TRAVELED DA WORLD!! @Bethanycsmithh üñ§ '\n",
    "sent = emoji.demojize(sent)\n",
    "sent = re.sub(r'https?:\\/\\/[^ \\n\\t\\r]*', 'URL', sent)\n",
    "sent = re.sub(r'@[a-zA-Z0-9_]+', 'USR', sent);\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocations are expressions of multiple words which commonly co-occur. They give important insight into the common patterns in both classes. A number of measures are available in NLTK to score collocations or other associations. The collocations in the corpus were then, scored based on those metrices and ranked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True).tokenize\n",
    "tokens = tokenizer('\\n'.join(corpus))\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "scored = finder.score_ngrams(bigram_measures.chi_sq)\n",
    "sorted(bigram for bigram, score in scored)\n",
    "map(lambda x: print(' '.join(x[0]), x[1], \"\\n\"), scored[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocation Scoring\n",
    "\n",
    "| Raw Freq | Chi_Sq/Dice/Jaccard/Phi_Sq/pmi     | Likelihood Ratio          |\n",
    "|----------|------------------------------------|---------------------------|\n",
    "| ! !      | #034i #100                         | ! !                       |\n",
    "| in the   | #100 #glitter                      | i love                    |\n",
    "| . i      | #1stphoto #rektek                  | I I                       |\n",
    "| i love   | #2003 2003                         | going to                  |\n",
    "| I I      | #2015isthenewturnup #myboos        | to be                     |\n",
    "| to be    | #2015season #2014sucks             | face_with_tears_of_joy :: |\n",
    "| of the   | #2am http://t.co/49XwyrlADo        | in the                    |\n",
    "| . I      | #2n1edition http://t.co/oRT6ZYfGhx | can't wait                |\n",
    "| for the  | #2o14 #bestie                      | :: face_with_tears_of_joy |\n",
    "| to the   | #2of6 #6daystretch                 | : Ô∏è                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer [this spreadsheet](https://docs.google.com/spreadsheets/d/1SzHaj5J7IedYPns2vLK2_3aX85TJNJwuLoMNe5Vz670/edit#gid=1691220959) to look at top 10 collocations using different metrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Implementations of traditional ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried out two different N-gram language models as features:\n",
    "1. [Unigram](http://www.nltk.org/_modules/nltk/model/ngram.html)\n",
    "2. [Bigram](http://www.nltk.org/_modules/nltk/model/ngram.html)\n",
    "\n",
    "Those featues were then, converted to one of the following encodings for training:\n",
    "1. [Bag of Words](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer)\n",
    "2. [Count](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)\n",
    "3. [Tf-idf](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "\n",
    "Following is the code for featurizing the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize(corpus):\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True).tokenize\n",
    "    vectorizer = TfidfVectorizer(strip_accents=\"unicode\", analyzer=\"word\", tokenizer=tokenizer, stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried out multiple ML classifiers with different features and also, did comparative study.\n",
    "The following classifiers were tried:\n",
    "\n",
    "1. [Naive Bayes](http://www.nltk.org/api/nltk.classify.html#module-nltk.classify.naivebayes)\n",
    "2. [Decision Tree](http://www.nltk.org/api/nltk.classify.html#module-nltk.classify.decisiontree)\n",
    "3. [MaxEnt](http://www.nltk.org/api/nltk.classify.html#module-nltk.classify.maxent)\n",
    "4. [GIS](http://www.nltk.org/api/nltk.classify.html#module-nltk.classify.maxent)\n",
    "5. [IIS](http://www.nltk.org/api/nltk.classify.html#module-nltk.classify.maxent)\n",
    "6. [Bernoulli Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)\n",
    "7. [Decision Tree Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "8. [Extra Trees Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)\n",
    "9. [Gausian Na√Øve Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "10. [Gradient Boosting Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "11. [K Nearest Neighbours Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "12. [Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "13. [Linear Support Vector Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)\n",
    "14. [Multinomial Na√Øve Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "15. [Nu-Support Vector Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html)\n",
    "16. [Random Forest Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "17. [Support Vector Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "\n",
    "Following is the code for using the classifiers and doing [10-fold cross-validation](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    K_FOLDS = 10 # 10-fold crossvalidation\n",
    "    CLF = LinearSVC() # the default, non-parameter optimized linear-kernel SVM\n",
    "    corpus, y = parse_dataset(DATASET_FP) # Loading dataset and featurised simple Tfidf-BoW model\n",
    "    X = featurize(corpus)\n",
    "    # Returns an array of the same size as 'y' where each entry is a prediction obtained by cross validated\n",
    "    predicted = cross_val_predict(CLF, X, y, cv=K_FOLDS)\n",
    "    score = metrics.f1_score(y, predicted, pos_label=1)\n",
    "    print (\"F1-score Task\", TASK, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer the code [here](../benchmark_system/example.py) for baselines and corpus analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Comparision\n",
    "\n",
    "In order to test the baselines, we trained many classifiers and evaluated them. For almost every classifier we also used three set of features: bag of words (presence), count and tf-idf. For every feature set we tried Unigram and Bigram model. For some classifier we tried hyper parameters to get the best accuracy. \n",
    "In order to see which one is the best accuracy:\n",
    "As we observe, bag of words in most cases is not a good selection. Also  we can observe that tf-idf is probably the best feature, implying that we should have less sparse and better embeddings as features.\n",
    "Unigrams, according to the figure below, shows better performance than bigrams. This holds for almost every classifier. \n",
    "Finally when comparing the classifiers, the best performance was obtained from Nu-Support Vector Classifier with 66.92%. The minimum scores was observed when using the Random forest classifier, which requires more parameter tuning to achieve comparable results.\n",
    "\n",
    "Refer to [this spreadsheet](https://docs.google.com/spreadsheets/d/1x-MsT3iaUSs85UQPOhBzIOWSe4DOmhXs7M9TqUs_QJ4/edit#gid=256991540) for complete comparision.\n",
    "\n",
    "\n",
    "### Bigram does not improve much\n",
    "<img src=\"Bigram_Unigram.png\" alt=\"Bigram vs Unigram\" title=\"Bigram vs Unigram\" />\n",
    "\n",
    "### Tf-idf performs the best\n",
    "\n",
    "<img src=\"Features_Extraction.png\" alt=\"Features Extraction\" title=\"Features Extraction\"/>\n",
    "\n",
    "### Nu-SVC, Naive Bayes and Logistic Regression give comparativeley better performance than all\n",
    "\n",
    "<img src=\"Baseline_Comparisions.png\" alt=\"Baseline Comparision\" title=\"Baseline Comparision\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also found the most informative features for the classifications to see, which words were important in distinguishing both class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_informative_feature_for_binary_classification(vectorizer, classifier, n=10):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.coef_[0], feature_names))[:n]\n",
    "    topn_class2 = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n",
    "\n",
    "    for coef, feat in topn_class1:\n",
    "        print (class_labels[0], coef, feat)\n",
    "    print ()\n",
    "    for coef, feat in reversed(topn_class2):\n",
    "        print (class_labels[1], coef, feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Naive Bayes                                                     | Logistic Regresssion                               | Linear SVC                        | MaxEnt                                        |\n",
    "|-----------------------------------------------------------------|-----------------------------------|-----------------------------------|-----------------------------------------------|\n",
    "| waking = 1               irony : non-ir =     14.4 : 1.0        | irony 3.43705831007 love          | irony 2.09798273149 unamused_face | 0.892 swag==1 and label is 'irony'            |\n",
    "| final = 1               irony : non-ir =      9.7 : 1.0         | irony 3.14001851865 great         | irony 1.9912945247 love           | 0.892 illridewithyou==1 and label is 'irony'  |\n",
    "| understand = 1              non-ir : irony  =      8.3 : 1.0    | irony 2.68216438685 fun           | irony 1.95849441798 great         | 0.866 speakeasy==1 and label is 'non-irony'   |\n",
    "| yay = 1               irony : non-ir =      7.6 : 1.0           | irony 2.10512791389 unamused_face | irony 1.89300320932 yay           | 0.631 socialist==1 and label is 'irony'       |\n",
    "| unamused_face = 1               irony : non-ir =      7.0 : 1.0 | irony 1.94194114863 yay           | irony 1.56713777847 joy           | -0.609 waking==1 and label is 'non-irony'     |\n",
    "| fix = 1               irony : non-ir =      6.4 : 1.0           | irony 1.75237273405 nice          | irony 1.55896446035 monday        | 0.604 amazeballs==1 and label is 'irony'      |\n",
    "| have = 2               irony : non-ir =      6.4 : 1.0          | irony 1.7355600331 thanks         | irony 1.52283732966 nice          | 0.604 peak==1 and label is 'non-irony'        |\n",
    "| check = 1              non-ir : irony  =      6.1 : 1.0         | irony 1.6065307335 wow            | irony 1.5100562397 fun            | 0.600 kmyb19hr==1 and label is 'non-irony'    |\n",
    "| wonderful = 1               irony : non-ir =      5.8 : 1.0     | irony 1.60091530592 oh            | non-irony -1.50574938325 check    | 0.597 subtweeting==1 and label is 'non-irony' |\n",
    "| dont = 1              non-ir : irony  =      5.8 : 1.0          | non-irony -1.57302071027 :        | irony 1.47746195477 test          | 0.593 bliss==1 and label is 'irony'           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic Property in Irony\n",
    "\n",
    "[Gibbs [1994]](http://psycnet.apa.org/record/1994-98510-000) states that verbal irony is a technique of using incongruity to suggest a distinction between reality and expectation. Incongruity is defined as the state of being incongruous (i.e. lacking in harmony; not in agreement with principles).\n",
    "\n",
    "<img src=\"CS269-diagram.png\" alt=\"Incongruity in Irony\" title=\"Incongruity in Irony\" />\n",
    "\n",
    "We try to capture this principle in our deep neurel net models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Model to Capture Linguistic Property of Irony"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our first approach is to use a neural network for the classification task. We use a simple multi-layer preceptron of 3 layers with 'Relu' and 'tanh' activations. The model was trained on both sentiment scores and word embeddings as features. The maximum length of the input sentences was fixed at 20 words. This enables us to use vanilla neural networks for the task. For extracting sentiment features, we used sentiwordnet to get an objective and subjective score for sentiments of individual words. We omitted words of neutral sentiments. This helps in making our model robust. A single vector formed by concatenating the sentiment scores is taken as a sentiment feature. Similarly, we add word embeddings of the individual words to incorporate semantic features. \n",
    "\n",
    "The idea here is that, a sarcastic/ironical utterance has various forms of incongruities. There can be semantic incongruity, where the meaning of one part of the sentence may contradict the other. Likewise, one can also have sentiment incongruity, where the sentiment of a word/phrase may contradict with the other. For example, consider \"I love bad news\". Here, \"love\" has positive sentiment, while \"bad\" has negative sentiment. Thus, capturing the difference between the sentiment polarities of various parts of the utterance can provide valuable information on the presence of sarcasm/irony. \n",
    "\n",
    "In our case, we considered sentiment incongruity between the words, but one can calculate phrase level and sentence level incongruity too.  With these features, a vanilla neural network of 3 layers was trained. The number of inputs were 40 (20 for word embeddings and 20 for sentiments). The hidden layers learn multiple levels of abstraction/feature representations which is used by the softmax layer at the top to predict the output class. 'Binary cross entropy' was used as the loss function. The use of other loss functions were found to yield poor results. The number of hidden units and the hidden layers are considered as hyper-parameters and optimized using cross-validation. The overall implementation is [here](../NN_system). Parts of our implementation are shown below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features, extracted using a tf-idf vectorizer and a Twitter tokenizer, are using the below function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature extraction using tf-idf\n",
    "def featurize(corpus):\n",
    "    # Tokenizes and creates TF-IDF BoW vectors.\n",
    "    # param corpus: A list of strings each string representing sentence.\n",
    "    # return: X: A sparse csr matrix of TFIDF-weigted ngram counts.\n",
    "\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True).tokenize\n",
    "    vectorizer = TfidfVectorizer(strip_accents=\"unicode\", analyzer=\"word\", tokenizer=tokenizer, stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    # print(vectorizer.get_feature_names()) # to manually check if the tokens are reasonable\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall architecture of our model is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our model\n",
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 20 neurons and relu activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1,name=\"Hidden1_activations\");\n",
    "    # Hidden fully connected layer with 10 neurons and tanh activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.tanh(layer_2,name=\"Hidden2_activations\");\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the training of the above model using AdamOptimizer is performed using the below tensorflow snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training the model using AdamOptimizer and softmax_cross_entropy loss\n",
    "logits = neural_net(X)\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "generator = gen(permute, batch_size);\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    for step in range(1, num_steps + 1):\n",
    "        batch_x, batch_y = generator.next()\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
    "            print(\"Step \"+str(step)+\", Minibatch Loss= \"+\"{:.4f}\".format(loss)+\", Training Accuracy= \"+\"{:.3f}\".format(acc))\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training accuracy   : ~94%\n",
    "\n",
    "Validation accuracy : ~72% (second position in the shared task SemEval Task3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final accuracy obtained by our model on the validation set is 72%. It is to be noted that the state-of-the-art accuracy for irony detection is ~88%. That model was attention-based RNN using around 100k examples, while we had only 3834 examples in our corpus. Hence, we plan to increase our model complexity with the addition of more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next idea was to use a recurrent neural network coupled with attention. The benefit of RNNs is that they can model the context of the inputs, which can be extremely helpful for sarcasm detection. For example, consider this scenario: 'A student forgot to complete his homework. The teacher says \"And that's how a homework should be done!\"' The second utterance, when stood alone, conveys a positive sentiment. But when we take the context into account, we can infer the underlying sarcasm in it. Thus, we used a recurrent neural network to model the overall context of our input sentences. The inputs to our recurrent neural network are word embeddings, which, as we have seen, can capture the lexical and semantic inconguities present in the inputs. \n",
    "\n",
    "The RNN is made up of LSTM units. The memory size of the LSTM unit was chosen to be 32 (out of trial and error). The maximum sequence length was chosen to be 20 (as it covers most of the examples). Word embeddings of dimensions 25 and 50 were tried. Then we applied attention on the LSTM outputs. This is to give different weights to the outputs of the LSTM, and use this weighted combination as features for classification. The outputs of the LSTMs are typically fed to a neural network, which outputs a probability distribution over the weights of the attention. The weighted combination is taken as a feature for the classification, along with the output state from the LSTM, which is used independent of the attention. The overall attention model is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/LSTM-attention.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above diagram, the h's are the outputs of the LSTM units, not shown in the image. The following are some of the snippets from the code. The more elaborate version can be found [here](https://github.com/jayanthjaiswal/SemEval2018-Task3/tree/master/attention_lstm). The snippet for applying attention over the LSTM outputs is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply attention\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:    # if True, assumes uniform weight over all dimensions of a particular LSTM output\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    # multiply element wise and retain the shape\n",
    "    output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    return output_attention_mul\n",
    "\n",
    "# extract the model\n",
    "def model_attention_applied_after_lstm():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    lstm_units = 32 # dimensionality of the output space / #LSTM units\n",
    "    lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)\n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The key component of the code for performing model training and validation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of training examples. In our case, 3834.\n",
    "N = Num_Training_Inputs;\n",
    "# get inputs from a data generator.\n",
    "inputs, outputs = get_data_recurrent(Num_Training_Inputs, TIME_STEPS, INPUT_DIM)\n",
    "if APPLY_ATTENTION_BEFORE_LSTM:\n",
    "    m = model_attention_applied_before_lstm()\n",
    "else:\n",
    "    m = model_attention_applied_after_lstm()\n",
    "\n",
    "# compile the model and train it.\n",
    "m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "m.fit([inputs], outputs, epochs=60, batch_size=20, validation_split=0.1,callbacks=[history])\n",
    "\n",
    "# plceholder for attention weights. for later visualization. \n",
    "attention_vectors = []\n",
    "for i in range(10):\n",
    "    testing_inputs, testing_outputs = get_data_recurrent(1, TIME_STEPS, INPUT_DIM)\n",
    "    attention_vector = np.mean(get_activations(m,testing_inputs,print_shape_only=True,\n",
    "                                               layer_name='attention_vec')[0], axis=2).squeeze()\n",
    "    print('attention =', attention_vector)\n",
    "    assert (np.sum(attention_vector) - 1.0) < 1e-5\n",
    "    attention_vectors.append(attention_vector)\n",
    "attention_vector_final = np.mean(np.array(attention_vectors), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "Training accuracy   : 90%\n",
    "\n",
    "Validation Accuracy : 62% (Overfitting, even with the simplest attention network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:\n",
    "While training the model, we observed that even the simplest attention based RNNs perform poorly when compared to the vanilla versions. We note that, this is due to the complexity of the model. The simplest of our attention network has ~12k parameters while the number of examples we have is ~4k. So, there is a high possibility of our model overfitting. Infact, that's what we observed while analysing the evolution of validation loss. The final accuracy we obtained on the validation set was 62%, which is much lower than many traditional baselines. In future, we intend to exploit additional datasets, to increase our model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Implementation and Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Boosting:\n",
    "Based on the history of the performance of boosting, we hypothesized that with the proper feature selection, boosting would achieve a significantly high accuracy score. In addition to the deep learning model, we explored a boosting implementation. The idea was to use the same word embedding features that were used in the DNN in the boosting algorithm to achieve a better performance. A variety of feature implementations were tested. The features included sentiment score extractions, word embeddings, and a combination of the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Score Features:\n",
    "\n",
    "It was hypothesized that ironic tweets would have words with starkly contrasting sentiment scores. Given a corpus, each tweet was tokenized. Each word in the tokenized list was passed through a sentiment analysis tool (nltk sentiwordnet) and the positive and negative scores of the word were kept track of. Looking at groups of 3 words at a time, the maximum positive sentiment score and the maximum negative sentiment score in a 3 word window was determined and the two values were subtracted to represent how \"far away\" the sentiments of these words were from each other. The number of 3-word chunks that were observed was a hyperparameter MAX_LEN_2 that was tuned(shown in the code below).\n",
    "\n",
    "The full code for boosting with sentiment score features can be found [here](https://github.com/jayanthjaiswal/SemEval2018-Task3/blob/master/xgboost_test/boost_test.py)\n",
    "\n",
    "The code listed below demonstrates how the sentiment features were constructed for each tweet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN_2 = 6\n",
    "def get_sent_max_pos_neg(corpus):\n",
    "    for curr_sentence in corpus:\n",
    "        curr_sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', ' ', curr_sentence)\n",
    "        curr_sentence = re.sub(r'@[a-zA-Z0-9_]+', ' ', curr_sentence)\n",
    "        sentence_tokenized = tokenizer(curr_sentence)\n",
    "        sent_list = []\n",
    "        for i in range(len(sentence_tokenized)-2):\n",
    "            curr_word = sentence_tokenized[i]\n",
    "            next_word = sentence_tokenized[i+1]\n",
    "            next_next = sentence_tokenized[i+2]\n",
    "            if curr_word.startswith('#'):\n",
    "                curr_word = curr_word[1:]\n",
    "            if next_word.startswith('#'):\n",
    "                next_word = next_word[1:]\n",
    "            if next_next.startswith('#'):\n",
    "                next_next = next_next[1:]\n",
    "            curr_senti_synsets = swn.senti_synsets(curr_word)\n",
    "            next_senti_synsets = swn.senti_synsets(next_word)\n",
    "            next_next_senti_synsets = swn.senti_synsets(next_next)\n",
    "            if len(curr_senti_synsets) > 0 and len(next_senti_synsets) > 0 and len(next_next_senti_synsets) > 0:\n",
    "                curr_pos = curr_senti_synsets[0].pos_score()\n",
    "                curr_neg = curr_senti_synsets[0].neg_score()\n",
    "                next_pos = next_senti_synsets[0].pos_score()\n",
    "                next_neg = next_senti_synsets[0].neg_score()\n",
    "                next_next_pos = next_next_senti_synsets[0].pos_score()\n",
    "                next_next_neg = next_next_senti_synsets[0].neg_score()\n",
    "                max_pos = max(curr_pos, next_pos, next_next_pos)\n",
    "                max_neg = max(curr_neg, next_neg, next_next_neg)\n",
    "                curr_sent = max_pos - max_neg\n",
    "                if curr_sent != 0:\n",
    "                    sent_list.append(curr_sent)\n",
    "        if len(sent_list) > MAX_LEN_2:\n",
    "            sent_list = sent_list[:MAX_LEN_2]\n",
    "        for i in range(MAX_LEN_2 - len(sent_list)):\n",
    "            sent_list.append(0.0)\n",
    "        inp_X.append(sent_list)\n",
    "    return inp_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding Features:\n",
    "\n",
    "Since simply using sentiment scores did not perform very well, exploring word embedding features was the next approach. We utilized the pre-trained word embeddings called GloVe from Stanford and outputted two sets of word embeddings. For the first set, each tweet corresponded to a text file where each row represented a word in the tweet using a 25d word embedding. For the second set, each tweet corresponded to a text file where each row represented a word in the tweet using a 50d word embedding.  \n",
    "\n",
    "The first task was to preprocess the text files by flattening each text file into a 1 dimensinal array representing the word embedding for approximately the first 10 words in a tweet. The number of words that were included in the flattened word embedding vector was a hyperparameter, MAX_LEN (shown in the code below). Each tweet example was represented as a 1  x 250 dimensional vector. This way, the examples were stacked together to get a full input training data matrix (data_X in the code below) with dimensions 3834 X 251 (last column was the labels). \n",
    "\n",
    "The same was done for the 50d word embeddings, resulting in an input training data matrix (data_X in the code below) with dimensions 3834 X 501. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting Implementation\n",
    "Once the data was preprocessed with the sentiment score features and word embedding features (stored in data_x in code below), the training data was split into X_train, X_test, y_train and y_test (as shown in the code below). Xgboost was run to fit a model to the training data. We used the XGBRegressor from the xgboost library. This was a classifier used a logistic regression model to perform the binary classification task. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 350\n",
    "test_size = 0.1\n",
    "X = data_x[:,0:MAX_LENGTH+MAX_LEN_2]\n",
    "Y = data_x[:,MAX_LENGTH+MAX_LEN_2]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=test_size)\n",
    "# fit model no training data\n",
    "model = XGBRegressor(max_depth=3) #gave 56.51%\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracies.append(accuracy)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full code for boosting with word embeddings and sentiment score features can be found [here](https://github.com/jayanthjaiswal/SemEval2018-Task3/blob/master/xgboost_test/boost_test_2.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering, Hyperparameters, Accuracy Analysis\n",
    "\n",
    "A combination of word embedding features and sentiment score features were used to analyze which would produce the best accuracy scores. The main parameters that were tuned in the model were MAX_LEN, MAX_LEN_2, test_size (representing the k_fold), and the max_depth of the decision trees in the boosting model. The best results were achieved with test_size = 0.1 and the max_depth = 3 for all models. For the models involving the 25d word embedding features, MAX_LEN = 350. For the models involving the 50d word embedding features, MAX_LEN = 550. For models involving sentiment scores MAX_LEN_2 = 6.\n",
    "\n",
    "The findings of the best scores along with the corresponding hyperparameters are summarized in the chart below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature Types                       | Accuracy | MAX_LEN | MAX_LEN_2 | test_size | max_depth |\n",
    "|-------------------------------------|----------|---------|-----------|-----------|-----------|\n",
    "| Only_Sentiment_Scores               | 54.95%   | n/a     | 6         | 0.1       | 3         |\n",
    "| Only_25d_Word_Embedding             | 61.20%   | 350     | n/a       | 0.1       | 3         |\n",
    "| Only_50d_Word_Embedding             | 61.98%   | 550     | n/a       | 0.1       | 3         |\n",
    "| 25d_Word_Embedding_Sentiment_Scores | 62.50%   | 350     | 6         | 0.1       | 3         |\n",
    "| 50d_Word_Embedding_Sentiment_Scores | 60.16%   | 550     | 6         | 0.1       | 3         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Even though we hypothesized that boosting would be a simple method to increase performance scores, it was observed that it was unable to beat the DNN approaches. The highest that was achieved using the boosting method was 62.50%, using 25d word embedding and sentiment score features. We concluded that in order to observe good performance for a boosting algorithm, proper feature selection is key. Feature selection in the task of classifying irony is complex. With more basic feature extractions, sophisticated systems such as DNNs perform better, as opposed to the boosting approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Analysis and Future Work\n",
    "\n",
    "Here is a final look at the results:\n",
    "\n",
    "| Model   Types                                   | Accuracy |\n",
    "|-------------------------------------------------|----------|\n",
    "| Na√Øve_Bayes_Unigram_Count                       | 65.66%   |\n",
    "| Logistic_Regression_Unigram_TFIDF               | 64.44%   |\n",
    "| Nu-SVC_RBF_Bigram_TFIDF                         | 66.92%   |\n",
    "| XGBoost_Sentiment_Scores                        | 54.95%   |\n",
    "| XGBoost_25d_Word_Embedding                      | 61.20%   |\n",
    "| XGBoost_50d_Word_Embedding                      | 61.98%   |\n",
    "| XGBoost_25d_Word_Embedding_Sentiment_Scores     | 62.50%   |\n",
    "| XGBoost_50d_Word_Embedding_Sentiment_Scores     | 60.16%   |\n",
    "| NN_Word_Embeddings                              | 71.52%   |\n",
    "| NN_Sentiment_Scores                             | 73.01%   |\n",
    "| **NN_Word_Embeddings_Sentiment_Scores**         |**73.67%**|\n",
    "| RNNs_Attention_Word_Embeddings_GloVe            | 65.05%   |\n",
    "| RNNs_Attention_Word_Embeddings_Word2Vec         | 62.80%   |\n",
    "| RNNs_Attention_Sentiment_Scores                 | 66.89%   |\n",
    "| RNNs_Attention_Word_Embeddings_Sentiment_Scores | 65.19%   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Features That Were Observed Through This Study: \n",
    "\n",
    "* The dataset was too small (3834 tweets), often leading to overfitting\n",
    "* Neural Nets with word embeddings and sentiment scores perform the best\n",
    "* For future work, we need to exploit tweet markers as specific features like hashtags and user anchors for more accuracy\n",
    "* Language incongruity can also be incorporated in the Neural Net model by training a tweet language model and comparing the model‚Äôs output with currently present word in tweet\n",
    "* Context incongruity can also be incorporated in the models by scraping the url to get the context in tweet dialouges or contextual referances.\n",
    "* Dataset has ambiguous and highly contextual irony labels like:\n",
    "    - [We want turkey!!](../datasets/train/irony-corpus-taskA/irony/116.txt)\n",
    "    - [@JordanNoftall that‚Äôs funny](../datasets/train/irony-corpus-taskA/irony/147.txt)\n",
    "\n",
    "Finally, we need to submit all the models in the SemEval System to see the performance on their hidden dataset, which will be released in January 15th."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
